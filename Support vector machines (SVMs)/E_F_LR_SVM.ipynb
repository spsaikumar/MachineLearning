{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HExLQrE4ZxR"
      },
      "source": [
        "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuKrFzC4ZxV"
      },
      "source": [
        "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wES-wWN4ZxX"
      },
      "source": [
        "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
        "\n",
        "Check the documentation for better understanding of these attributes: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
        "\n",
        "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
        "\n",
        "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
        "\n",
        "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
        "\n",
        "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
        "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
        "\n",
        "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
        "\n",
        "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z830CfMk4Zxa"
      },
      "source": [
        "## Task E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuBxHiCQ4Zxc"
      },
      "source": [
        "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
        "\n",
        "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
        "\n",
        "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCgMNEvI4Zxf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANUNIqCe4Zxn"
      },
      "source": [
        "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2d3UbrXf9fM",
        "outputId": "24e54045-c615-43c4-dabd-55c9d7916584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.35375589,  0.28929301,  0.48523022,  0.62203148, -1.24936153],\n",
              "       [ 0.20425535,  1.60431078, -0.065753  , -0.00474915,  1.54888018],\n",
              "       [-1.29765409, -0.5344866 ,  0.23614881,  0.27004474, -1.17486639],\n",
              "       ...,\n",
              "       [-0.04510433,  0.60958112, -0.0476168 , -0.03016537,  0.65812123],\n",
              "       [-0.16984873,  0.0415595 ,  0.23741361,  0.29950873, -0.69521718],\n",
              "       [ 1.29596578,  0.89516891, -0.14080382, -0.13311413,  1.18441842]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGwKUkUugGxb",
        "outputId": "1ddc5319-0fdf-4c8c-f1b6-6576db4f0aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHie1zqH4Zxt"
      },
      "source": [
        "### Pseudo code\n",
        "\n",
        "clf = SVC(gamma=0.001, C=100.)<br>\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
        "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
        "    \n",
        "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
        "\n",
        "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h43kDT3M41u5"
      },
      "source": [
        "# you can write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x, x_test, y, y_test = train_test_split(X, y,test_size=0.2,train_size=0.8)\n",
        "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)"
      ],
      "metadata": {
        "id": "M34UzZPOcDfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train.shape, x_test.shape,x_cv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9xtA-uxc1RK",
        "outputId": "e5024a08-8f8c-4f9c-8930-7bfd1acb0afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3000, 5), (1000, 5), (1000, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SVC(gamma=0.001, C=100)\n",
        "clf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLIMN-4Yc7DW",
        "outputId": "c943e84c-bf96-4700-8307-159ce7495648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, gamma=0.001)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.get_params, clf.kernel, clf.intercept_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxSNqaOUc8Oe",
        "outputId": "2e93037c-9244-4c2d-b572-ccf691952632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<bound method BaseEstimator.get_params of SVC(C=100, gamma=0.001)>,\n",
              " 'rbf',\n",
              " array([-1.57646877]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get parameters from model\n",
        "params = clf.get_params()\n",
        "sv = clf.support_vectors_\n",
        "nv = clf.n_support_\n",
        "a  = clf.dual_coef_ # this is alpha only for support vectors, for nonsupport vectors alpha = 0\n",
        "b  = clf.intercept_\n",
        "cs = clf.classes_"
      ],
      "metadata": {
        "id": "mBiDNmlgc8hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(x_cv))\n",
        "print(len(x_cv))\n",
        "print(x_cv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuSw0H0Nhyps",
        "outputId": "461b5a0f-6f7c-44a6-c1f2-0f4ca57f510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "1000\n",
            "[[ 0.48215731  1.24143829  0.46790447  0.64640229 -0.39684066]\n",
            " [ 0.35211152 -0.36235531  0.33215966  0.39868435 -1.3256336 ]\n",
            " [-1.00159064 -0.82853651  0.13823665  0.1331221  -1.12059188]\n",
            " ...\n",
            " [ 0.82585588  0.54889609  0.29837644  0.40045354 -0.45682887]\n",
            " [ 1.26343018  1.83624705  0.6896265   0.95302406 -0.57939965]\n",
            " [-2.07298311  0.17832065  0.12230291  0.16188511 -0.22642606]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYdTP7pIl_cf",
        "outputId": "0c1f1864-1aff-411f-e922-0cbbf7b458ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
              "       0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
              "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(cs))\n",
        "print(len(cs))\n",
        "print(cs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdC6r1Vrg-SH",
        "outputId": "e7ae5c8f-9d29-449e-89fb-f1e1c3432b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "2\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(b))\n",
        "print(len(b))\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQVFgko0gy_j",
        "outputId": "f2ffdeb5-00c8-4468-fbbd-0e827df71cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "1\n",
            "[-1.57646877]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(a))\n",
        "print(len(a), len(a[0]))\n",
        "print(a[0][0:10])\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M19oA78ige7u",
        "outputId": "9baaed59-8f27-43fd-d664-ba7c48f2ef45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "1 542\n",
            "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n",
            "[[-100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.           -6.26824106\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.          -20.16371122 -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.         -100.         -100.         -100.         -100.\n",
            "  -100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.           44.95610038  100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.           39.17982659\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.           56.18751398\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.           86.10851133\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.          100.          100.          100.\n",
            "   100.          100.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(nv))\n",
        "print(len(nv))\n",
        "print(nv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIJiNoshgaID",
        "outputId": "496ab6de-921d-4123-e62c-4cac57bbcf2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "2\n",
            "[271 271]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(sv))\n",
        "print(len(sv), len(sv[0]))\n",
        "print(sv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpN6ihHOgRJk",
        "outputId": "51444693-3729-4a82-9e43-5c9dc898c2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "542 5\n",
            "[[ 1.19477638  0.74569145  0.01688183  0.0572437   0.57402515]\n",
            " [-0.07649339 -0.52811347 -0.13047041 -0.18904866 -0.04207648]\n",
            " [-1.1777564   1.58633609  0.18955473  0.31430098  0.7486608 ]\n",
            " ...\n",
            " [-0.19555522 -0.46192596  0.05727139  0.04940943 -0.56386817]\n",
            " [ 0.36843345 -0.83770548 -0.11838942 -0.18889369 -0.33910295]\n",
            " [ 0.26188892  0.50694812 -0.03744594 -0.02238768  0.54069231]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(params))\n",
        "print(len(params))\n",
        "print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URX-VmRffeBX",
        "outputId": "c95c1de9-bd6f-4aa5-e744-db0d05a7d8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "15\n",
            "{'C': 100, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.001, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_function(x_cv):\n",
        "    final_val = []\n",
        "    for xq in x_cv:\n",
        "        sum = 0\n",
        "        for i in range(len(sv)):   \n",
        "          \n",
        "          l2_norm = np.linalg.norm(xq - sv[i])\n",
        "          pow = np.exp(-params['gamma'] * (l2_norm ** 2))\n",
        "          sum +=  a[0][i] * pow  \n",
        "        v = sum + clf.intercept_\n",
        "        final_val.append(v[0])\n",
        "    return final_val"
      ],
      "metadata": {
        "id": "BtYn9faVc8wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_bound_val = decision_function(x_cv)"
      ],
      "metadata": {
        "id": "_Hnf8v3Rc87w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_bound_val[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEdmzrhTc9Jq",
        "outputId": "c02cd65b-f925-4029-900d-9f44725e7210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-2.3038072356976578,\n",
              " -3.7731801150086817,\n",
              " -2.9212521019248463,\n",
              " -2.56241082905397,\n",
              " -3.3188490899262284,\n",
              " -1.3015962185337278,\n",
              " -1.884638865812956,\n",
              " -0.6672390341749652,\n",
              " -2.7258135872456357,\n",
              " 2.0438469838631796]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute f_cv value by using sklearn\n",
        "f_cv_sklearn = clf.decision_function(x_cv)\n",
        "f_cv_sklearn[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_SswCP7c9at",
        "outputId": "25d60c38-fb54-4446-905c-226f5db58060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.30380724, -3.77318012, -2.9212521 , -2.56241083, -3.31884909,\n",
              "       -1.30159622, -1.88463887, -0.66723903, -2.72581359,  2.04384698])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Hence, **f_cv_sklearn ==  decision_bound_val**"
      ],
      "metadata": {
        "id": "k6Zq5M4MnHBY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bKCboN4Zxu"
      },
      "source": [
        "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMn7OEN94Zxw"
      },
      "source": [
        "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
        "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n5EFkx4Zxz"
      },
      "source": [
        "## TASK F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HOqVJq4Zx1"
      },
      "source": [
        "\n",
        "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
        "\n",
        "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
        "\n",
        "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
        "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
        "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
        "\n",
        "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyyvVYJlxMCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTY7z2bd4Zx2"
      },
      "source": [
        "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9lzOhNwxOfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM3odN1Z4Zx3"
      },
      "source": [
        "\n",
        "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
        "\n",
        "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
        "\n",
        "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
        "\n",
        "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
        "\n",
        "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to initialize weights\n",
        "def initialize_weights(dim):\n",
        "  ''' In this function, we will initialize our weights and bias''' \n",
        "  #initialize the weights to zeros array of (1,dim) dimensions\n",
        "  #you use zeros_like function to initialize zero, check this link https:// #initialize bias to zero\n",
        "  w = np.zeros_like(dim)\n",
        "  b=0\n",
        "  return w,b "
      ],
      "metadata": {
        "id": "OTih-D49xqrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to compute sigmoid value\n",
        "def sigmoid(z):\n",
        "  ''' In this function, we will return sigmoid of z'''\n",
        "  # compute sigmoid(z) and return\n",
        "  sig_z=1/(1+(np.exp(-z )))\n",
        "  return sig_z"
      ],
      "metadata": {
        "id": "D_qqOKDUx7PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to compute log-loss value\n",
        "def logloss(y_true,y_pred):\n",
        "  '''In this function, we will compute log loss '''\n",
        "  n = len(y_true) \n",
        "  s=0\n",
        "  for i in range(n):\n",
        "    t = y_true[i]*np.log10(y_pred[i])+ (1.0-y_true[i])*np.log10(1.0-y_pred[i])\n",
        "    s=s+t\n",
        "  loss = ((-1.0) / n) * s \n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "XAp4HboryC-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to gradient weights value\n",
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "  '''In this function, we will compute the gardient w.r.to w '''\n",
        "  dw =x*(y-sigmoid(np.dot(w,x)+b)) - ((alpha*w)/N)\n",
        "  return dw"
      ],
      "metadata": {
        "id": "q89rsxi1yS_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to compute gradient intercept value\n",
        "def gradient_db(x,y,w,b):\n",
        "  '''In this function, we will compute gradient w.r.to b '''\n",
        "  db = y-sigmoid(np.dot(w,x)+b) \n",
        "  return db"
      ],
      "metadata": {
        "id": "ZaUOXsU1ydXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plat_scaling(y_train , y_cv): \n",
        "  y_cv_plat= []\n",
        "  plus= ( np.count_nonzero(y_train==1)) \n",
        "  minus= ( np.count_nonzero(y_train==0))\n",
        "  # plus = np.count_nonzero(y_train)\n",
        "  # minus = len(y_train) - np.count_nonzero(y_train)\n",
        "  y_plus= (plus+1)/(plus+2) \n",
        "  y_minus=1/(minus+2)\n",
        "  for i in range(len(y_cv)):\n",
        "    if y_cv[i] == 1: \n",
        "      y_cv_plat.append(y_plus)\n",
        "    if y_cv[i] == 0: \n",
        "      y_cv_plat.append(y_minus)\n",
        "  return np.array(y_cv_plat) \n",
        "\n",
        "y_cv_plat=plat_scaling(y_train,y_cv) \n",
        "print(y_cv_plat[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFgcYmL_xPGA",
        "outputId": "f4b78570-9fe6-4701-fc58-e85851aebe41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.77554919e-04 4.77554919e-04 4.77554919e-04 4.77554919e-04\n",
            " 4.77554919e-04 4.77554919e-04 9.98901099e-01 9.98901099e-01\n",
            " 4.77554919e-04 9.98901099e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train,y_train,epochs,alpha,eta0):\n",
        "  ''' In this function, we will implement logistic regression'''\n",
        "  #Here eta0 is learning rate\n",
        "  #implement the code as follows\n",
        "  # initalize the weights call the initialize_weights(X_train[0] function \n",
        "  w,b = initialize_weights(X_train[0])\n",
        "  train_loss = []\n",
        "  test_loss = []\n",
        "  # for every epoch\n",
        "  for epoch in range(0,epochs):\n",
        "    # for every data point(X_train,y_train)\n",
        "    ypred_train = []\n",
        "    ypred_test = []\n",
        "    for x,y in zip(X_train,y_train):\n",
        "      #compute gradient w.r.to w (call the gradient_dw() function)\n",
        "      dw = gradient_dw(x,y,w,b,alpha,len(X_train))\n",
        "      #compute gradient w.r.to b (call the gradient_db() function) \n",
        "      db = gradient_db(x,y,w,b)\n",
        "      #update w, b\n",
        "      w += eta0*dw\n",
        "      b += eta0*db\n",
        "\n",
        "    # predict the output of x_train[for all data points in X_train] using\n",
        "    for x in X_train: \n",
        "      ypred_train.append(sigmoid(np.dot(w,x) + b))\n",
        "    \n",
        "    #compute the loss between predicted and actual values \n",
        "    tr_loss = logloss(y_train,ypred_train)\n",
        "\n",
        "    # append all the train loss values in a list \n",
        "    train_loss.append(logloss(y_train,ypred_train))\n",
        "    # predict the output of x_test[for all data points in X_test] using w \n",
        "    \n",
        "    for x in x_test:\n",
        "      ypred_test.append(sigmoid(np.dot(w,x) + b))\n",
        "    \n",
        "    #compute the loss between predicted and actual values  \n",
        "    te_loss = logloss(y_test, ypred_test)\n",
        "    # store all the test loss values in a list \n",
        "    test_loss.append(logloss(y_test,ypred_test)) \n",
        "    \n",
        "  return w, b, train_loss"
      ],
      "metadata": {
        "id": "A4IuLjBh68_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(x_train)\n",
        "epochs= 50 \n",
        "w,b,train_loss=train(decision_function(x_cv),y_cv_plat,epochs,alpha,eta0) \n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A61EjUZ-sDp",
        "outputId": "c0c05852-5e5a-42bb-b095-f1a00d22b629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1664065425107293\n",
            "-0.15598305238755233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epoch_lst = [i for i in range(1, 51)]\n",
        "plt.plot(epoch_lst,train_loss,label = 'Train_loss') \n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('Epoch Number VS Train Loss') \n",
        "plt.xlabel('Epoch Number') \n",
        "plt.ylabel('Train Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UA3WgbwG_JYt",
        "outputId": "3c0001c3-4855-457d-86c1-dff8c8cb1cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_test=clf.decision_function(x_test)\n",
        "f_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwWankFX_tvc",
        "outputId": "19c6f1db-7fd4-445f-f7d3-ae03ed3e63a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.34123274, -1.56640185,  1.22607421, -3.95053852, -2.74862018,\n",
              "       -1.40902312,  1.69911625, -2.8539968 ,  0.80039616, -2.93981534])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
      ],
      "metadata": {
        "id": "6L6D0YJNAMqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_value_list = [] \n",
        "for x_q in f_test:\n",
        "  res=1/(1+np.exp(-w*x_q-b)) \n",
        "  prob_value_list.append(res)\n",
        "print(prob_value_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAssHU0Q_0HE",
        "outputId": "78f1f5db-6f7b-436d-cb14-6e9f9c8a5c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05280934488490812,\n",
              " 0.12099467488560041,\n",
              " 0.7814468542762315,\n",
              " 0.008459941996148012,\n",
              " 0.03350467051023587,\n",
              " 0.1419147615401363,\n",
              " 0.8612698586219156,\n",
              " 0.02974483479796233,\n",
              " 0.685163549197528,\n",
              " 0.02698803131841617,\n",
              " 0.06565955182571563,\n",
              " 0.00531407520782944,\n",
              " 0.2861476254234422,\n",
              " 0.03030166900850771,\n",
              " 0.1556514423562347,\n",
              " 0.0009515467563463711,\n",
              " 0.07683731560071187,\n",
              " 0.5066736056431985,\n",
              " 0.9259421829132217,\n",
              " 0.01966578932825423,\n",
              " 0.33780305189572757,\n",
              " 0.059440461588572434,\n",
              " 0.0243343916065324,\n",
              " 0.17400236820915962,\n",
              " 0.011709479720719777,\n",
              " 0.0950018696220894,\n",
              " 0.08266000539480318,\n",
              " 0.03527464909829068,\n",
              " 0.021173306588073877,\n",
              " 0.047140881702497146,\n",
              " 0.5108521034894725,\n",
              " 0.01821836439426924,\n",
              " 0.9542750006405921,\n",
              " 0.13225167324526327,\n",
              " 0.28588023599874834,\n",
              " 0.46537789087152237,\n",
              " 0.11007380967566113,\n",
              " 0.8303677264241748,\n",
              " 0.946545879776031,\n",
              " 0.7022580800912552,\n",
              " 0.4201805257015786,\n",
              " 0.983315657627977,\n",
              " 0.07508600118783158,\n",
              " 0.04727442466638495,\n",
              " 0.9091863296805651,\n",
              " 0.042384555423445054,\n",
              " 0.014554508819674485,\n",
              " 0.6354320465700032,\n",
              " 0.7924208062258395,\n",
              " 0.033015161715218004,\n",
              " 0.953161307438751,\n",
              " 0.6337608673804249,\n",
              " 0.03622414653849298,\n",
              " 0.16566315843183094,\n",
              " 0.19395391127056008,\n",
              " 0.6217192009386817,\n",
              " 0.8762073672640607,\n",
              " 0.05494741858336276,\n",
              " 0.25284503316349716,\n",
              " 0.07092098941025488,\n",
              " 0.054682608376993146,\n",
              " 0.0381334266397644,\n",
              " 0.9422015438929868,\n",
              " 0.05926430199877578,\n",
              " 0.9398472475782191,\n",
              " 0.049778386263989845,\n",
              " 0.03594510495000403,\n",
              " 0.0300881462742261,\n",
              " 0.01162798056771067,\n",
              " 0.02240587118588954,\n",
              " 0.04661165251210143,\n",
              " 0.025904806413828148,\n",
              " 0.04695727626747077,\n",
              " 0.5531327462706132,\n",
              " 0.07916657798477406,\n",
              " 0.9167222690733963,\n",
              " 0.024474161810563982,\n",
              " 0.20165476810656693,\n",
              " 0.9165091462736412,\n",
              " 0.07079261797044581,\n",
              " 0.08090403105487894,\n",
              " 0.027074363617729014,\n",
              " 0.038840839982786,\n",
              " 0.0995581431881114,\n",
              " 0.9280479037272691,\n",
              " 0.6956769938283397,\n",
              " 0.8548617434932108,\n",
              " 0.030696925112923764,\n",
              " 0.04121512793773993,\n",
              " 0.7413436447747059,\n",
              " 0.07246070709885315,\n",
              " 0.15324283970652952,\n",
              " 0.2732422414425263,\n",
              " 0.3304868968460666,\n",
              " 0.013193759295225522,\n",
              " 0.029425634886171476,\n",
              " 0.792420125610828,\n",
              " 0.8746320682430879,\n",
              " 0.23450384175217745,\n",
              " 0.8617683740009108,\n",
              " 0.6424629095438963,\n",
              " 0.728652026646254,\n",
              " 0.026700571073730958,\n",
              " 0.032981996444345,\n",
              " 0.5730479286544937,\n",
              " 0.20846403396575763,\n",
              " 0.05403916158781522,\n",
              " 0.757050696588287,\n",
              " 0.8917791135163913,\n",
              " 0.3323788727783448,\n",
              " 0.8983493253044619,\n",
              " 0.4145256161948812,\n",
              " 0.055343843821562634,\n",
              " 0.008390116116541748,\n",
              " 0.05667706589220528,\n",
              " 0.8406421743009861,\n",
              " 0.02190050767674116,\n",
              " 0.1225756199322304,\n",
              " 0.01648436068834962,\n",
              " 0.8429549446026273,\n",
              " 0.28333908562780824,\n",
              " 0.7154065462120917,\n",
              " 0.024815544780049277,\n",
              " 0.012717952618661195,\n",
              " 0.14803096736254018,\n",
              " 0.589961824548742,\n",
              " 0.9304852391242857,\n",
              " 0.00605257122893609,\n",
              " 0.02421964266162792,\n",
              " 0.0012800312263763819,\n",
              " 0.11493021836315596,\n",
              " 0.10482781433549149,\n",
              " 0.9274676588703834,\n",
              " 0.02523331049662554,\n",
              " 0.007131710021084351,\n",
              " 0.7599984087133485,\n",
              " 0.9348480997965221,\n",
              " 0.7787290449289517,\n",
              " 0.07241405073328573,\n",
              " 0.9104379031946152,\n",
              " 0.011191666516234263,\n",
              " 0.9864104932976354,\n",
              " 0.0905420389842514,\n",
              " 0.009052284824464747,\n",
              " 0.8458370966298916,\n",
              " 0.8399719459412692,\n",
              " 0.25446183006329853,\n",
              " 0.9203415715172104,\n",
              " 0.01740163094689993,\n",
              " 0.08905025554052128,\n",
              " 0.08650225388861184,\n",
              " 0.11907842736991557,\n",
              " 0.026457483759852983,\n",
              " 0.012790616538025068,\n",
              " 0.9700192071890137,\n",
              " 0.9045901322022739,\n",
              " 0.1074259618905279,\n",
              " 0.01583050793068828,\n",
              " 0.000681414193128445,\n",
              " 0.028756685443590835,\n",
              " 0.9642428263797191,\n",
              " 0.07189091986450492,\n",
              " 0.06883925929081064,\n",
              " 0.062222317420873585,\n",
              " 0.09156635063970085,\n",
              " 0.0423309519386541,\n",
              " 0.7869653902834978,\n",
              " 0.3456290376002935,\n",
              " 0.036292862975124476,\n",
              " 0.2083461950458505,\n",
              " 0.37457642892580456,\n",
              " 0.889896763776053,\n",
              " 0.12288284901514472,\n",
              " 0.3845497774789992,\n",
              " 0.00705885642528757,\n",
              " 0.00992839738598758,\n",
              " 0.36145393789389174,\n",
              " 0.8196633392170681,\n",
              " 0.3412037401462443,\n",
              " 0.03331445573650871,\n",
              " 0.8812160346769367,\n",
              " 0.010259130632018245,\n",
              " 0.006803938686477084,\n",
              " 0.5184475107922765,\n",
              " 0.9220566953310757,\n",
              " 0.06108814371955486,\n",
              " 0.09382765178910871,\n",
              " 0.03549306615922268,\n",
              " 0.8765691884753393,\n",
              " 0.9471760637140841,\n",
              " 0.8710296562827172,\n",
              " 0.021866462545538125,\n",
              " 0.05921557916513279,\n",
              " 0.008180459521588091,\n",
              " 0.8304587098504328,\n",
              " 0.029216928118647224,\n",
              " 0.2631917055229202,\n",
              " 0.07898083029214617,\n",
              " 0.09282428297117833,\n",
              " 0.8773489817950095,\n",
              " 0.014539162771305156,\n",
              " 0.8618716571445388,\n",
              " 0.7713839509407387,\n",
              " 0.7488994646256425,\n",
              " 0.8781055538374629,\n",
              " 0.01704430492222716,\n",
              " 0.01738503704146066,\n",
              " 0.08674967466227156,\n",
              " 0.2936916923043546,\n",
              " 0.020416265671393378,\n",
              " 0.08727765580752209,\n",
              " 0.03211084524915205,\n",
              " 0.017595536592034677,\n",
              " 0.3425998979551329,\n",
              " 0.8466568144660904,\n",
              " 0.016636474873121723,\n",
              " 0.8946685407650103,\n",
              " 0.8854401876888036,\n",
              " 0.01824440811961498,\n",
              " 0.34424565314068245,\n",
              " 0.012266612652155782,\n",
              " 0.8625207538657708,\n",
              " 0.08029258651164095,\n",
              " 0.8742357828873157,\n",
              " 0.04983464671221924,\n",
              " 0.0802236031880772,\n",
              " 0.05070315022108578,\n",
              " 0.42843212178764195,\n",
              " 0.003308514966207199,\n",
              " 0.07428887294399099,\n",
              " 0.8904905005893666,\n",
              " 0.9417225758732207,\n",
              " 0.12075196616095475,\n",
              " 0.01810776474164495,\n",
              " 0.5983529676834708,\n",
              " 0.5538202268702752,\n",
              " 0.11723768417709265,\n",
              " 0.029207641533511693,\n",
              " 0.8370699458329263,\n",
              " 0.3339656508281593,\n",
              " 0.12485312011342166,\n",
              " 0.03673674127188635,\n",
              " 0.021084162661352893,\n",
              " 0.011247798736882876,\n",
              " 0.023983029743287497,\n",
              " 0.0025188093946887573,\n",
              " 0.16497223120850865,\n",
              " 0.011869077639625705,\n",
              " 0.8681808319314862,\n",
              " 0.004903960024128727,\n",
              " 0.4010784644054665,\n",
              " 0.41243255907964366,\n",
              " 0.06367438529760923,\n",
              " 0.5774286139487854,\n",
              " 0.23257934929733923,\n",
              " 0.007130558391645388,\n",
              " 0.8073208473772072,\n",
              " 0.5067797117644024,\n",
              " 0.023253845693712114,\n",
              " 0.04220948324930393,\n",
              " 0.834486090223281,\n",
              " 0.8047977596126249,\n",
              " 0.03970496967103815,\n",
              " 0.6856248376152139,\n",
              " 0.09302616665291316,\n",
              " 0.2006238030892827,\n",
              " 0.009975959222532424,\n",
              " 0.7071328526303489,\n",
              " 0.10884791626069011,\n",
              " 0.07501372423065623,\n",
              " 0.045785372752473075,\n",
              " 0.026409797190787825,\n",
              " 0.00825746396180601,\n",
              " 0.010746549066058863,\n",
              " 0.022669251537660553,\n",
              " 0.012397292752164012,\n",
              " 0.8490627121029816,\n",
              " 0.031217680120541044,\n",
              " 0.05049573472081687,\n",
              " 0.012861946460415835,\n",
              " 0.19705924735983082,\n",
              " 0.004313195294385329,\n",
              " 0.11924275450256733,\n",
              " 0.026153925775396054,\n",
              " 0.004072200288354141,\n",
              " 0.6938048309855059,\n",
              " 0.04404092309710525,\n",
              " 0.5823135843450845,\n",
              " 0.6631749250039493,\n",
              " 0.8078025503233114,\n",
              " 0.0015732229074065511,\n",
              " 0.4781191477899574,\n",
              " 0.8679292556246542,\n",
              " 0.875776716665213,\n",
              " 0.06633679742473621,\n",
              " 0.016963096848663862,\n",
              " 0.13620471141321513,\n",
              " 0.26951959042627777,\n",
              " 0.2455873875888092,\n",
              " 0.7017292837111191,\n",
              " 0.027666626997037196,\n",
              " 0.007194238255482872,\n",
              " 0.029407278372082974,\n",
              " 0.08035318790758503,\n",
              " 0.9769680935761783,\n",
              " 0.49118677795099236,\n",
              " 0.8497710755570612,\n",
              " 0.6400938150535397,\n",
              " 0.027200001485745876,\n",
              " 0.17734052037905412,\n",
              " 0.08349486702294935,\n",
              " 0.8146381941679983,\n",
              " 0.050913240719359494,\n",
              " 0.9106366135097196,\n",
              " 0.8906548487340289,\n",
              " 0.8754212621932539,\n",
              " 0.026311376741235674,\n",
              " 0.03953606136919972,\n",
              " 0.17993114232295238,\n",
              " 0.7288608323698238,\n",
              " 0.06346378048511025,\n",
              " 0.2745254134193893,\n",
              " 0.017342380360991623,\n",
              " 0.6826616404482642,\n",
              " 0.06299952705840114,\n",
              " 0.1351069889040485,\n",
              " 0.01397213336081256,\n",
              " 0.04589540066972227,\n",
              " 0.013790980720739874,\n",
              " 0.92364733818662,\n",
              " 0.9499417076129814,\n",
              " 0.42206108554328003,\n",
              " 0.030752811496248988,\n",
              " 0.07034949989492245,\n",
              " 0.13843206251545648,\n",
              " 0.6485063457823683,\n",
              " 0.09017531504088845,\n",
              " 0.05570078033221064,\n",
              " 0.04371513119234179,\n",
              " 0.4447663255753327,\n",
              " 0.11511843186406512,\n",
              " 0.12535663328373167,\n",
              " 0.012789493583923893,\n",
              " 0.016566198782625167,\n",
              " 0.04658601586750717,\n",
              " 0.9268946739815249,\n",
              " 0.014416162069079844,\n",
              " 0.031416643155321576,\n",
              " 0.7719316304775193,\n",
              " 0.07687139329857352,\n",
              " 0.18197332936129412,\n",
              " 0.8955888303125383,\n",
              " 0.007407690630102357,\n",
              " 0.03280428693559363,\n",
              " 0.3219352856209907,\n",
              " 0.8689110765310223,\n",
              " 0.03138425206245252,\n",
              " 0.9335967715791742,\n",
              " 0.11129807755194844,\n",
              " 0.7697119742514664,\n",
              " 0.04800599051951683,\n",
              " 0.06519680796902445,\n",
              " 0.12246737484007984,\n",
              " 0.012096718477339284,\n",
              " 0.11697650495199065,\n",
              " 0.006563069781579291,\n",
              " 0.08562563570267819,\n",
              " 0.8841792067065506,\n",
              " 0.03990881548398383,\n",
              " 0.1826345204291714,\n",
              " 0.01772242833772275,\n",
              " 0.01578592349435886,\n",
              " 0.006447144172403568,\n",
              " 0.9097063011948416,\n",
              " 0.9363982289937903,\n",
              " 0.8844727909888305,\n",
              " 0.014948145553251597,\n",
              " 0.018049533896542156,\n",
              " 0.6544198933840093,\n",
              " 0.7761951556972925,\n",
              " 0.007059692733031717,\n",
              " 0.9060799888557999,\n",
              " 0.024106002964833912,\n",
              " 0.07694997126749502,\n",
              " 0.015250743436573923,\n",
              " 0.10606627087940078,\n",
              " 0.029540106587024457,\n",
              " 0.016610068689972387,\n",
              " 0.013960260329267078,\n",
              " 0.04681108288157669,\n",
              " 0.8655651805485489,\n",
              " 0.31175593750962494,\n",
              " 0.04279421663466938,\n",
              " 0.9757045143538317,\n",
              " 0.20304817618035126,\n",
              " 0.5448158166999142,\n",
              " 0.034388458080824434,\n",
              " 0.8293159344056902,\n",
              " 0.9250210130871571,\n",
              " 0.015238226936062022,\n",
              " 0.10871953917681608,\n",
              " 0.6962977275905451,\n",
              " 0.08746565797492242,\n",
              " 0.9769872144304772,\n",
              " 0.9217753646164657,\n",
              " 0.19440866432130058,\n",
              " 0.08083044242838161,\n",
              " 0.8988550562798495,\n",
              " 0.009700363975655428,\n",
              " 0.30227574327645135,\n",
              " 0.8957879668634804,\n",
              " 0.13157880185371637,\n",
              " 0.05172482242792658,\n",
              " 0.5833716091864557,\n",
              " 0.009337724660924113,\n",
              " 0.9575341795105861,\n",
              " 0.2905492549988558,\n",
              " 0.06708544643971599,\n",
              " 0.014027839365071732,\n",
              " 0.10370274099227264,\n",
              " 0.7057371390944106,\n",
              " 0.05284696589994696,\n",
              " 0.048793900293195115,\n",
              " 0.019665308403295605,\n",
              " 0.908833717009145,\n",
              " 0.029918992267755226,\n",
              " 0.02384474877575256,\n",
              " 0.013115405968704733,\n",
              " 0.02762266197510174,\n",
              " 0.08548981082429212,\n",
              " 0.015946969579927427,\n",
              " 0.6681603267591616,\n",
              " 0.0634930201507613,\n",
              " 0.06715444085824561,\n",
              " 0.04254907545811491,\n",
              " 0.020502361895776953,\n",
              " 0.010713623667721635,\n",
              " 0.0059131971733638175,\n",
              " 0.06138765070337642,\n",
              " 0.9234157545849533,\n",
              " 0.4376340870289961,\n",
              " 0.37952566052488335,\n",
              " 0.07730411650789004,\n",
              " 0.8582655378747275,\n",
              " 0.7841124524379054,\n",
              " 0.022971051139300272,\n",
              " 0.9044069865093558,\n",
              " 0.2188001615083703,\n",
              " 0.029141024949420304,\n",
              " 0.03993031392430485,\n",
              " 0.010968843407003916,\n",
              " 0.7020156102760523,\n",
              " 0.03223095061420569,\n",
              " 0.03271989216110564,\n",
              " 0.019181304837420483,\n",
              " 0.7276770175337341,\n",
              " 0.9278321246066848,\n",
              " 0.01393031735978025,\n",
              " 0.05672693723337157,\n",
              " 0.1217456460503153,\n",
              " 0.02785832838810232,\n",
              " 0.40868605385955953,\n",
              " 0.005782800669030566,\n",
              " 0.007347121253087992,\n",
              " 0.005322526845472422,\n",
              " 0.026205242136699294,\n",
              " 0.12492849183308208,\n",
              " 0.006335637783824448,\n",
              " 0.5356207405575049,\n",
              " 0.10055904069184778,\n",
              " 0.6859392719334896,\n",
              " 0.08080508375838404,\n",
              " 0.039246573503050594,\n",
              " 0.07115192202595466,\n",
              " 0.021480169081934532,\n",
              " 0.03655951090459882,\n",
              " 0.8597484789867238,\n",
              " 0.75057610076356,\n",
              " 0.014194285059625117,\n",
              " 0.06087180978678486,\n",
              " 0.029480339113588716,\n",
              " 0.10773683682205223,\n",
              " 0.18125246885868845,\n",
              " 0.06477571593118124,\n",
              " 0.22549889549913676,\n",
              " 0.054067920812589626,\n",
              " 0.06311507875864204,\n",
              " 0.010603490267116178,\n",
              " 0.014600117058559115,\n",
              " 0.4162165635995029,\n",
              " 0.2528139397630757,\n",
              " 0.03513951401753099,\n",
              " 0.02752314094923159,\n",
              " 0.4602742473524269,\n",
              " 0.1919497181565301,\n",
              " 0.7925024348416094,\n",
              " 0.9612268971714609,\n",
              " 0.1426770302714739,\n",
              " 0.07129093554436045,\n",
              " 0.3986231651284073,\n",
              " 0.10742012888759006,\n",
              " 0.12972727370495188,\n",
              " 0.30266703436750536,\n",
              " 0.010375293950844562,\n",
              " 0.0792534583767736,\n",
              " 0.9206196130509376,\n",
              " 0.6949156701400767,\n",
              " 0.1893214933636919,\n",
              " 0.9047217451412135,\n",
              " 0.03170223582365991,\n",
              " 0.8091041924167097,\n",
              " 0.01810338384805992,\n",
              " 0.027721303771326332,\n",
              " 0.8683031784154869,\n",
              " 0.7171969933580234,\n",
              " 0.3144457363198339,\n",
              " 0.328033946409235,\n",
              " 0.1441648882098209,\n",
              " 0.09147613436893694,\n",
              " 0.18873892862658465,\n",
              " 0.14301543903546032,\n",
              " 0.012479433487619323,\n",
              " 0.8497449238187287,\n",
              " 0.06604695834996811,\n",
              " 0.5316945842551625,\n",
              " 0.0014684419548563806,\n",
              " 0.005024089364890728,\n",
              " 0.025062975779566125,\n",
              " 0.8181609296729507,\n",
              " 0.26640344219546697,\n",
              " 0.010006536825277837,\n",
              " 0.019139911638489777,\n",
              " 0.06544333455441427,\n",
              " 0.02298919417684434,\n",
              " 0.033474412259135874,\n",
              " 0.1436022583665937,\n",
              " 0.06402411376232325,\n",
              " 0.9482578758354012,\n",
              " 0.7143794017259981,\n",
              " 0.020811673429188982,\n",
              " 0.029729615739531687,\n",
              " 0.015958499386885858,\n",
              " 0.20934866758835252,\n",
              " 0.009013843216338394,\n",
              " 0.024970811160245257,\n",
              " 0.07372268178210663,\n",
              " 0.06851956090083991,\n",
              " 0.010795613119719336,\n",
              " 0.024155167815551798,\n",
              " 0.7214799144863765,\n",
              " 0.009025672408671087,\n",
              " 0.013177809305320583,\n",
              " 0.4118720118395196,\n",
              " 0.0716058931776423,\n",
              " 0.7285414304578803,\n",
              " 0.8834523637171536,\n",
              " 0.013197151492478552,\n",
              " 0.863456405173703,\n",
              " 0.10934263962160319,\n",
              " 0.0893733822861438,\n",
              " 0.3122673031749341,\n",
              " 0.047173340006030066,\n",
              " 0.0403057534728932,\n",
              " 0.19521420345617263,\n",
              " 0.17778076008933047,\n",
              " 0.029100962939619092,\n",
              " 0.23579544679868752,\n",
              " 0.19906906555555318,\n",
              " 0.03366738623330204,\n",
              " 0.05732157058233698,\n",
              " 0.005470187574294601,\n",
              " 0.13098949318653993,\n",
              " 0.8547976891483402,\n",
              " 0.014194343886716499,\n",
              " 0.9369154535669086,\n",
              " 0.4298865703047375,\n",
              " 0.0349420948955819,\n",
              " 0.2244574448887589,\n",
              " 0.9384212483626106,\n",
              " 0.01149628183081563,\n",
              " 0.1098199793462035,\n",
              " 0.8859629091020459,\n",
              " 0.09613243849270961,\n",
              " 0.021641074986108247,\n",
              " 0.10121118226717063,\n",
              " 0.013433628982826303,\n",
              " 0.04122685714705559,\n",
              " 0.05218113915159401,\n",
              " 0.8351660066566664,\n",
              " 0.010873858708669635,\n",
              " 0.14814977042163138,\n",
              " 0.11675538868165042,\n",
              " 0.010810141182403612,\n",
              " 0.0008643077495609074,\n",
              " 0.8313123711761972,\n",
              " 0.05420531929843326,\n",
              " 0.08089277171538689,\n",
              " 0.01997613126038782,\n",
              " 0.9068103336994527,\n",
              " 0.023429604177026324,\n",
              " 0.9482762627969038,\n",
              " 0.9318807593871867,\n",
              " 0.7324905109102037,\n",
              " 0.8494277804240623,\n",
              " 0.041871991031308044,\n",
              " 0.8745515937681574,\n",
              " 0.08802753220649089,\n",
              " 0.08745861250864885,\n",
              " 0.8362663054624704,\n",
              " 0.016439835896503545,\n",
              " 0.06895084955772321,\n",
              " 0.009641026227206044,\n",
              " 0.9037420102808491,\n",
              " 0.9021177983086781,\n",
              " 0.9735028935396803,\n",
              " 0.03575097799017844,\n",
              " 0.0257948961030905,\n",
              " 0.03563819358475092,\n",
              " 0.6524252480424982,\n",
              " 0.8772363493948799,\n",
              " 0.6368503416095597,\n",
              " 0.15894712132552363,\n",
              " 0.010391598007983829,\n",
              " 0.46024409584070897,\n",
              " 0.08522756792939172,\n",
              " 0.8545784872360123,\n",
              " 0.06769259510040702,\n",
              " 0.6575035098682049,\n",
              " 0.9104882612385236,\n",
              " 0.03025432061137005,\n",
              " 0.14944193870697503,\n",
              " 0.004940156650716171,\n",
              " 0.015405285078706036,\n",
              " 0.918566041296628,\n",
              " 0.08243849356529846,\n",
              " 0.13828936607798942,\n",
              " 0.013292038485132902,\n",
              " 0.9492035910808753,\n",
              " 0.0626361100137615,\n",
              " 0.5839087784926575,\n",
              " 0.09805228498838085,\n",
              " 0.042048990608539005,\n",
              " 0.14506175177484443,\n",
              " 0.03236480836695139,\n",
              " 0.8142043228959837,\n",
              " 0.1415031070996431,\n",
              " 0.05509730290268157,\n",
              " 0.7050456270983607,\n",
              " 0.042298050216244124,\n",
              " 0.01172692638234856,\n",
              " 0.11342212988488073,\n",
              " 0.18269046624104257,\n",
              " 0.6852266063097935,\n",
              " 0.8756154194132247,\n",
              " 0.8911461930442008,\n",
              " 0.21883812945323483,\n",
              " 0.05068101687649634,\n",
              " 0.10871365799887914,\n",
              " 0.12767935840477324,\n",
              " 0.5654789291356431,\n",
              " 0.016198290425337013,\n",
              " 0.032229998830170054,\n",
              " 0.010870955060654903,\n",
              " 0.9724116598033204,\n",
              " 0.027688495130099987,\n",
              " 0.8330810700184861,\n",
              " 0.21484882281961665,\n",
              " 0.09041519976806306,\n",
              " 0.03924463397330075,\n",
              " 0.01364373355368444,\n",
              " 0.879908864548514,\n",
              " 0.2961463902743858,\n",
              " 0.05305113609383742,\n",
              " 0.15629130158139218,\n",
              " 0.2089571508784821,\n",
              " 0.0420622318436505,\n",
              " 0.907076261688528,\n",
              " 0.7454236491600239,\n",
              " 0.020615784570267214,\n",
              " 0.02677768540588784,\n",
              " 0.15675084437608905,\n",
              " 0.06849713799626136,\n",
              " 0.7993442176705381,\n",
              " 0.867872711253828,\n",
              " 0.047061427618317817,\n",
              " 0.03302557935204854,\n",
              " 0.09041624294884996,\n",
              " 0.8946092020742245,\n",
              " 0.251395820137031,\n",
              " 0.20637778586520117,\n",
              " 0.033876395279139486,\n",
              " 0.028604350504478043,\n",
              " 0.026538738828070334,\n",
              " 0.025928932078059908,\n",
              " 0.8868644062967077,\n",
              " 0.5737658497660817,\n",
              " 0.9539335761776099,\n",
              " 0.06997628133200721,\n",
              " 0.33841235715619117,\n",
              " 0.04473516927168656,\n",
              " 0.1682275228210059,\n",
              " 0.06888260592462739,\n",
              " 0.45744212963094855,\n",
              " 0.006382356208903918,\n",
              " 0.7444730974031196,\n",
              " 0.8433204181022982,\n",
              " 0.9301925435727018,\n",
              " 0.02244372230168354,\n",
              " 0.047852541541855614,\n",
              " 0.006094316279631271,\n",
              " 0.030640853701845102,\n",
              " 0.1237599118910755,\n",
              " 0.9008256863932598,\n",
              " 0.05816910831931679,\n",
              " 0.4229505569537014,\n",
              " 0.9068310259810438,\n",
              " 0.03109692965720284,\n",
              " 0.018049573181343728,\n",
              " 0.029716484938161015,\n",
              " 0.5077417350634534,\n",
              " 0.07557681164374629,\n",
              " 0.8790123812491383,\n",
              " 0.11868036400606127,\n",
              " 0.9674396396135502,\n",
              " 0.10259812932123259,\n",
              " 0.016807872140444755,\n",
              " 0.9018389488889279,\n",
              " 0.06653427853497916,\n",
              " 0.027226598896447997,\n",
              " 0.006941745804767229,\n",
              " 0.03317860135370531,\n",
              " 0.3764337718667936,\n",
              " 0.02406239092821029,\n",
              " 0.03705896030464828,\n",
              " 0.02448226935119304,\n",
              " 0.10499504809122596,\n",
              " 0.02243389524778425,\n",
              " 0.03184728623456166,\n",
              " 0.8572489361539496,\n",
              " 0.02828136762513412,\n",
              " 0.07743800256109024,\n",
              " 0.8654312305066685,\n",
              " 0.09709502548902546,\n",
              " 0.9905486391838888,\n",
              " 0.7131111804080634,\n",
              " 0.7760507520118105,\n",
              " 0.04820237721674795,\n",
              " 0.12540353483580433,\n",
              " 0.02619196498000114,\n",
              " 0.303318709648054,\n",
              " 0.11944886828837271,\n",
              " 0.011977049763765037,\n",
              " 0.27975402315787007,\n",
              " 0.03884711122823971,\n",
              " 0.10193792228003996,\n",
              " 0.012755319942596696,\n",
              " 0.23463618774074652,\n",
              " 0.7781914904460271,\n",
              " 0.09472662072266116,\n",
              " 0.06097668200809211,\n",
              " 0.06411086981540325,\n",
              " 0.0655285722518823,\n",
              " 0.03531675962296446,\n",
              " 0.03173519367027671,\n",
              " 0.19643741695168637,\n",
              " 0.34060424779972576,\n",
              " 0.1706211202605665,\n",
              " 0.8939691518161516,\n",
              " 0.10706703456379589,\n",
              " 0.037421278771268225,\n",
              " 0.879241593562424,\n",
              " 0.017732103089290893,\n",
              " 0.017577629517434266,\n",
              " 0.5595153627169216,\n",
              " 0.0951557108913035,\n",
              " 0.024689213713001014,\n",
              " 0.027944981515991844,\n",
              " 0.06909167779133235,\n",
              " 0.21006854008559944,\n",
              " 0.8964305665023571,\n",
              " 0.7848891668650231,\n",
              " 0.09873113166796449,\n",
              " 0.9763741178215501,\n",
              " 0.8726079723133744,\n",
              " 0.13532631838648623,\n",
              " 0.1649789271676795,\n",
              " 0.04363505860155907,\n",
              " 0.8277744280317796,\n",
              " 0.02005677102201386,\n",
              " 0.016378644348398468,\n",
              " 0.03740415373241329,\n",
              " 0.2683907324723421,\n",
              " 0.154299378105431,\n",
              " 0.8531912294602595,\n",
              " 0.9448813964443301,\n",
              " 0.865104203861441,\n",
              " 0.12830386714358158,\n",
              " 0.4904870965705373,\n",
              " 0.2115858482583723,\n",
              " 0.02272500973668741,\n",
              " 0.12615895609746738,\n",
              " 0.004020576635536729,\n",
              " 0.045810521102838724,\n",
              " 0.009377776394013868,\n",
              " 0.5558044838680444,\n",
              " 0.9057094186116976,\n",
              " 0.10639594900056595,\n",
              " 0.8288172657075955,\n",
              " 0.04763312212144033,\n",
              " 0.15405513842162713,\n",
              " 0.9376060867496833,\n",
              " 0.009142543992858375,\n",
              " 0.0773079834930051,\n",
              " 0.856138926667485,\n",
              " 0.8398848610314116,\n",
              " 0.8896327222134571,\n",
              " 0.1953259057785542,\n",
              " 0.8623410733904382,\n",
              " 0.885336441651743,\n",
              " 0.013330892551434511,\n",
              " 0.37443527355662914,\n",
              " 0.23056421198691907,\n",
              " 0.6125935176037062,\n",
              " 0.21068667800431606,\n",
              " 0.21949788061977632,\n",
              " 0.107141435227059,\n",
              " 0.10838770598930576,\n",
              " 0.007950236523883448,\n",
              " 0.03744757927979804,\n",
              " 0.8750643827519775,\n",
              " 0.4950604221921472,\n",
              " 0.7992284414035413,\n",
              " 0.2801945406071851,\n",
              " 0.4946056158044106,\n",
              " 0.08699582109227999,\n",
              " 0.9193049427016583,\n",
              " 0.043550876434427094,\n",
              " 0.016096532577819466,\n",
              " 0.8539266873826474,\n",
              " 0.010614469258281925,\n",
              " 0.04137702368370194,\n",
              " 0.7404507190237253,\n",
              " 0.0116478627988536,\n",
              " 0.06335730224360483,\n",
              " 0.03047487380678421,\n",
              " 0.20743872362642152,\n",
              " 0.6441499792352445,\n",
              " 0.24594004404046652,\n",
              " 0.0019800956955642436,\n",
              " 0.4377856368744055,\n",
              " 0.013769460167646169,\n",
              " 0.8273592611449986,\n",
              " 0.08300301444014897,\n",
              " 0.8250946053037016,\n",
              " 0.9250303469253837,\n",
              " 0.20656389767487915,\n",
              " 0.9709180847192673,\n",
              " 0.6469735426754288,\n",
              " 0.9421074792360654,\n",
              " 0.8691775301693193,\n",
              " 0.023395170856535752,\n",
              " 0.029925739844681724,\n",
              " 0.8349191265316758,\n",
              " 0.1355594144305163,\n",
              " 0.3028134263834574,\n",
              " 0.006867908275367315,\n",
              " 0.03900701105127447,\n",
              " 0.01493909512473077,\n",
              " 0.013873472354357479,\n",
              " 0.9415805487999035,\n",
              " 0.021742316707719155,\n",
              " 0.06594601057442237,\n",
              " 0.8782409829032828,\n",
              " 0.2523659414960167,\n",
              " 0.03675576769753401,\n",
              " 0.5555940006566416,\n",
              " 0.8597500713281024,\n",
              " 0.9085292169887685,\n",
              " 0.9016747124908516,\n",
              " 0.8476432975420057,\n",
              " 0.021185309741246938,\n",
              " 0.08279349933975814,\n",
              " 0.36806194654083546,\n",
              " 0.9712208067953348,\n",
              " 0.0460152818437369,\n",
              " 0.25743545251013045,\n",
              " 0.5688534742736094,\n",
              " 0.002129690203066187,\n",
              " 0.025482898688909617,\n",
              " 0.2785819353565093,\n",
              " 0.3463341317584468,\n",
              " 0.9147671022486896,\n",
              " 0.09804656577382832,\n",
              " 0.0704681851115403,\n",
              " 0.014659905217100457,\n",
              " 0.10859314367441807,\n",
              " 0.02843929154978785,\n",
              " 0.8829269015152238,\n",
              " 0.03638197751570202,\n",
              " 0.007896041730929627,\n",
              " 0.035534712003746076,\n",
              " 0.9133329720181609,\n",
              " 0.8464570644636009,\n",
              " 0.7774456538116593,\n",
              " 0.035076479070742614,\n",
              " 0.06541602982234285,\n",
              " 0.10328857474597264,\n",
              " 0.04193917898626649,\n",
              " 0.02670959317635597,\n",
              " 0.09294279464885238,\n",
              " 0.015855582647038527,\n",
              " 0.40566329401118634,\n",
              " 0.11680856719856977,\n",
              " 0.8965735335456564,\n",
              " 0.9014613605793677,\n",
              " 0.23426831381440025,\n",
              " 0.23622778641094658,\n",
              " 0.8927370009375435,\n",
              " 0.041988815048102854,\n",
              " 0.04351123511010187,\n",
              " 0.08926480896299374,\n",
              " 0.2962227099653602,\n",
              " 0.027203206654660632,\n",
              " 0.22418710169526285,\n",
              " 0.00820398214715405,\n",
              " 0.822596079197596,\n",
              " 0.1685981653412573,\n",
              " 0.8159888179525912,\n",
              " 0.07842310186444944,\n",
              " 0.04354536655323418,\n",
              " 0.040956829825984135,\n",
              " 0.5574380089497472,\n",
              " 0.8416964746195571,\n",
              " 0.004389317651069941,\n",
              " 0.047117945309060855,\n",
              " 0.026624154161739366,\n",
              " 0.47036623403823474,\n",
              " 0.923502424053808,\n",
              " 0.03466782159256127,\n",
              " 0.02743522232110602,\n",
              " 0.7345607192835063,\n",
              " 0.023142540659918567,\n",
              " 0.8985991496301374,\n",
              " 0.03991080233537106,\n",
              " 0.03726317457562501,\n",
              " 0.015695463244982798,\n",
              " 0.8980516111971808,\n",
              " 0.9613148704885872,\n",
              " 0.7854773636759083,\n",
              " 0.2671231791963051,\n",
              " 0.035503839876599934,\n",
              " 0.8385066394883812,\n",
              " 0.9207019598235167,\n",
              " 0.041802561415386505,\n",
              " 0.8202023779569848,\n",
              " 0.051434583564409685,\n",
              " 0.06089422761198065,\n",
              " 0.033787315841055175,\n",
              " 0.014449704265873038,\n",
              " 0.049583154011375925,\n",
              " 0.9316574939098964,\n",
              " 0.8088630019523657,\n",
              " 0.05635345228197503,\n",
              " 0.9519179811322532,\n",
              " 0.007821941775084454,\n",
              " 0.07920034999397153,\n",
              " 0.8365888637378718,\n",
              " 0.050836486525812255,\n",
              " 0.021751895787838735,\n",
              " 0.06084557477961439,\n",
              " 0.08054176392773842,\n",
              " 0.8307766533328139,\n",
              " 0.912289827206826,\n",
              " 0.31157708085696034,\n",
              " 0.09939574849796792,\n",
              " 0.8587818616048267,\n",
              " 0.9195954766464652,\n",
              " 0.07916077603652338,\n",
              " 0.07887891108516638,\n",
              " 0.47004736511243145,\n",
              " 0.09899751411304916,\n",
              " 0.01182828017481802,\n",
              " 0.041149642691645526,\n",
              " 0.060594472252919314,\n",
              " 0.80707009948222,\n",
              " 0.6801335458736024,\n",
              " 0.5593558811284938,\n",
              " 0.8847883952263595,\n",
              " 0.011181269873137945,\n",
              " 0.06192343704181304,\n",
              " 0.8047331759238687,\n",
              " 0.5665502301791604,\n",
              " 0.014032182364062763,\n",
              " 0.6944009521169697,\n",
              " 0.040078864341885304,\n",
              " 0.03619852893407212,\n",
              " 0.0575248502415954,\n",
              " 0.8057577151917928,\n",
              " 0.04296107310362809,\n",
              " 0.05544368059973392]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N3LWa90mACy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}